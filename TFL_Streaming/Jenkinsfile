pipeline {
    agent any

    triggers {
        cron('H/5 * * * *')  // runs pipeline every 5 minutes
    }

    parameters {
        booleanParam(name: 'RUN_PRODUCER_ONLY', defaultValue: true, description: 'Run only the producer stage?')
    }

    environment {
        PYSPARK_PYTHON = "/usr/bin/python3"
        PYSPARK_DRIVER_PYTHON = "/usr/bin/python3"
    }

    stages {
        stage('Upgrade pip') {
            when { expression { !params.RUN_PRODUCER_ONLY } } // skip if only producer run
            steps {
                sh '''
                    python3 -m pip install --upgrade pip setuptools wheel
                '''
            }
        }

        stage('Install Python dependencies') {
            when { expression { !params.RUN_PRODUCER_ONLY } }
            steps {
                sh '''
                    python3 -m pip install --user -r TFL_Streaming/requirements.txt
                '''
            }
        }

        stage('Run Producer (one-off)') {
            steps {
                sh '''
                    echo "Running single-run TFL producer..."
                    spark-submit \
                      --master local \
                      --packages "org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.7","com.lihaoyi:requests_2.11:0.7.1" \
                      TFL_Streaming/src/producer.py 2>&1 | tee logs/producer.log
                '''
            }
        }

//         stage('Start Consumer (streaming)') {
//             when { expression { !params.RUN_PRODUCER_ONLY } }
//             steps {
//                 sh '''
//                     echo "Starting streaming consumer in background..."
//                     nohup spark-submit \
//                       --master local \
//                       --packages "org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.7" \
//                       TFL_Streaming/src/consumer.py 2>&1 | tee logs/consumer.log &
//                     echo $! > consumer.pid
//                     echo "Consumer started with PID $(cat consumer.pid)"
//                 '''
//             }
//         }

        stage('Start Consumer (streaming)') {
            when { expression { !params.RUN_PRODUCER_ONLY } }
            steps {
                sh '''
                    echo "Starting streaming consumer (blocking, logs visible in Jenkins)..."
                    spark-submit \
                      --master local \
                      --packages "org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.7" \
                      TFL_Streaming/src/consumer.py 2>&1 | tee logs/consumer.log
                '''
            }
        }

        stage('Refresh Impala Metadata') {
            when { expression { !params.RUN_PRODUCER_ONLY } }
            steps {
                sh '''
                    impala-shell -q "REFRESH streaming_tfl_db.tfl_streaming_arrivals;"
                '''
            }
        }
    }

    post {
        always {
            sh '''
                echo "Jenkins pipeline finished."
            '''
        }
    }
}
